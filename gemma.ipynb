{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518b18ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sduailab\\Aimat\\research_1\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoTokenizer, Gemma3ForConditionalGeneration\n",
    "\n",
    "model_id = \"google/gemma-3-27b-it\"\n",
    "save_path = \"./gemma-3-27b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ee2ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 12/12 [00:38<00:00,  3.22s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gemma-3-27b-it\\\\tokenizer_config.json',\n",
       " './gemma-3-27b-it\\\\special_tokens_map.json',\n",
       " './gemma-3-27b-it\\\\chat_template.jinja',\n",
       " './gemma-3-27b-it\\\\tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Gemma3ForConditionalGeneration.from_pretrained(model_id)\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "processor.save_pretrained(save_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ded6c490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 22/22 [01:27<00:00,  3.96s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "local_path = \"./gemma-3-27b-it\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    local_path,\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65219c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are smart assistant\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What do you see?\n",
      "model\n",
      "Here's what I see in the image:\n",
      "\n",
      "**Main Subject:**\n",
      "\n",
      "*   Cristiano Ronaldo is the central figure. He's a well-known Portuguese professional footballer.\n",
      "*   He is holding a Ballon d'Or trophy. This is an annual football award presented to the best male player in the world.\n",
      "\n",
      "**Attire:**\n",
      "\n",
      "*   He's wearing a white Real Madrid football kit, including the jersey and shorts. The number \"7\" is clearly visible on his shorts.\n",
      "*   He has long sleeves rolled up.\n",
      "\n",
      "**Background:**\n",
      "\n",
      "*   The background is a grassy field, likely a football pitch.\n",
      "*   The background is slightly blurred, which focuses attention on Ronaldo.\n",
      "\n",
      "**Overall Impression:**\n",
      "\n",
      "The photo appears to be taken shortly after Ronaldo received the Ballon d'Or, and he's likely on the field acknowledging fans or celebrating.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(\"./gemma-3-27b-it\")\n",
    "gen_config.cache_implementation = \"dynamic\" \n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are smart assistant\"}]},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\", \"image\": Image.open(\"C:/Users/sduailab/Aimat/research_1/C._Ronaldo_-_Ballon_d'Or_2014.jpg\")}, \n",
    "        {\"type\": \"text\", \"text\": \"What do you see?\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,\n",
    "    return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "\n",
    "output = model.generate(**inputs, generation_config=gen_config, max_new_tokens=200)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca9bb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sduailab\\AppData\\Local\\Temp\\ipykernel_24488\\1052465098.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"C:/Users/sduailab/Aimat/research_1/MY20-AMG_GT_Group_WebPDF_091719.pdf\")  \n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectordb = Chroma.from_documents(texts, embedding, persist_directory=\"./rag_db\")\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e521c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 22/22 [00:39<00:00,  1.80s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, Gemma3ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    \"./gemma-3-27b-it\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./gemma-3-27b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "666f807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sduailab\\AppData\\Local\\Temp\\ipykernel_24488\\1432690455.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Контекст:\n",
      "AMG GT R PRO COUPE\n",
      "Handcrafted 577-hp AMG V8 \n",
      "biturbo engine\n",
      "designo  Selenite Grey Magno  \n",
      "Grey 19\"/20\" AMG® forged  \n",
      " twin 5 -spoke wheels1\n",
      "33\n",
      "AMG GT R PRO COUPE\n",
      "Handcrafted 577-hp AMG V8 \n",
      "biturbo engine\n",
      "designo  Selenite Grey Magno  \n",
      "Grey 19\"/20\" AMG® forged  \n",
      " twin 5 -spoke wheels1\n",
      "33\n",
      "AMG GT R PRO COUPE\n",
      "Handcrafted 577-hp AMG V8 \n",
      "biturbo engine\n",
      "designo  Selenite Grey Magno  \n",
      "Grey 19\"/20\" AMG® forged  \n",
      " twin 5 -spoke wheels1\n",
      "33\n",
      "\n",
      "Вопрос: Что за двигатель в AMG GT?\n",
      "Ответ: Двигатель в AMG GT - это 577-сильный AMG V8 с двойным турбонаддувом.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Что за двигатель в AMG GT?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "context = \"\\n\".join([doc.page_content for doc in docs[:3]])\n",
    "\n",
    "prompt = f\"Контекст:\\n{context}\\n\\nВопрос: {query}\\nОтвет:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=200)\n",
    "\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a78485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
